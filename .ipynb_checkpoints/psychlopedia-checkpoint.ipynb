{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import re\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from str_helper import *\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Populate definitions dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apa = pd.read_csv('ontologies\\APAONTO.csv').fillna('') #Preferred Label, Definitions, Alt name\n",
    "mf = pd.read_csv('ontologies\\MF.csv').fillna('')\n",
    "mfo = pd.read_csv('ontologies\\MFOMD.csv').fillna('')\n",
    "sui = pd.read_csv('ontologies\\suicideo.csv').fillna('')\n",
    "\n",
    "\n",
    "thes=defaultdict(dict)\n",
    "\n",
    "def add_defn(w, dfn, onto):\n",
    "    if w=='':\n",
    "        pass\n",
    "    d=thes[w]\n",
    "    d[onto]=re.sub(r'\\[wikipedia: http\\S+\\]', '', dfn.replace('\\n', ' '), flags=re.IGNORECASE)\n",
    "    thes[w] = d\n",
    "\n",
    "\n",
    "for row in apa.iterrows():\n",
    "    word = row[1]['Preferred Label']\n",
    "    defn = row[1]['Definitions']\n",
    "    if defn=='':\n",
    "        continue\n",
    "    syn = [word]+row[1]['Alt name'].split('|')\n",
    "    for w in syn:\n",
    "        add_defn(w, defn, 'apa')    \n",
    "        \n",
    "for row in mf.iterrows():\n",
    "    word = row[1]['Preferred Label']\n",
    "    defn = row[1]['Definitions']\n",
    "    if defn=='':\n",
    "        continue\n",
    "    add_defn(word, defn, 'mf')\n",
    "    \n",
    "for row in mfo.iterrows():\n",
    "    word = row[1]['Preferred Label']\n",
    "    defn = row[1]['Definitions']\n",
    "    if defn=='':\n",
    "        continue\n",
    "    syn = [word]+row[1]['Synonyms'].split('|')\n",
    "    for w in syn:\n",
    "        add_defn(w, defn, 'mfo')\n",
    "\n",
    "for row in sui.iterrows():\n",
    "    word = row[1]['Preferred Label']\n",
    "    defn = row[1]['Definitions']\n",
    "    if defn=='':\n",
    "        continue\n",
    "    syn = [word]+row[1]['alternative term'].split('\\n')\n",
    "    for w in syn:\n",
    "        add_defn(w, defn, 'sui')\n",
    "        \n",
    "\n",
    "with open('psychThesaurusDict.pkl','wb') as f:\n",
    "        pickle.dump(thes, f)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocab for hyperspace training - psychlopedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADHD ['Attention', 'deficit', 'hyperactivity', 'disorder']\n",
      "(ADHD) Attention deficit hyperactivity disorder\n",
      "ADD ['Attention', 'deficit', 'disorder\"']\n",
      "(ADD) Attention deficit disorder\"\n",
      "NHS ['National', 'Health', 'Service']\n",
      "(NHS) National Health Service\n",
      "PER ['so', 'named', 'Period']\n",
      "TIM ['Period', 'PER', 'Timeless']\n",
      "CRY ['Timeless', 'TIM', 'Cryptochrome']\n",
      "PER ['so', 'named', 'Period']\n",
      "TIM ['Period', 'PER', 'Timeless']\n",
      "CRY ['Timeless', 'TIM', 'Cryptochrome']\n",
      "Pavlovian ['classical']\n",
      "LEARNING ['the', 'aid', 'of', 'external', 'cues', 'Compare', 'SERIAL', 'ANTICIPATION']\n",
      "LEARNING ['the', 'aid', 'of', 'external', 'cues', 'Compare', 'SERIAL', 'ANTICIPATION']\n",
      "LEARNING ['to', 'a', 'new', 'context', 'or', 'setting', 'Compare', 'TRANSFER']\n",
      "TESTING ['established', 'scales', 'or', 'standards', 'Consider', 'also', 'SCORING']\n",
      "ATP ['to', 'form', 'triphosphates']\n",
      "ATP ['to', 'form', 'triphosphates']\n",
      "EVENTS ['change', 'due', 'to', 'illness', 'Compare', 'EXPERIENCES']\n",
      "EVENTS ['change', 'due', 'to', 'illness', 'Compare', 'EXPERIENCES']\n",
      "EMOTION ['think', 'and', 'thoughts', 'of', 'suicide', 'Use', 'DEPRESSION']\n",
      "EMOTION ['think', 'and', 'thoughts', 'of', 'suicide', 'Use', 'DEPRESSION']\n",
      "EMOTION ['think', 'and', 'thoughts', 'of', 'suicide', 'Use', 'DEPRESSION']\n",
      "EMOTION ['think', 'and', 'thoughts', 'of', 'suicide', 'Use', 'DEPRESSION']\n",
      "EMOTION ['think', 'and', 'thoughts', 'of', 'suicide', 'Use', 'DEPRESSION']\n",
      "EMOTION ['think', 'and', 'thoughts', 'of', 'suicide', 'Use', 'DEPRESSION']\n",
      "EMOTION ['think', 'and', 'thoughts', 'of', 'suicide', 'Use', 'DEPRESSION']\n",
      "EMOTION ['think', 'and', 'thoughts', 'of', 'suicide', 'Use', 'DEPRESSION']\n",
      "CONCEPTS ['and', 'the', 'operations', 'performed', 'on', 'them', 'Compare', 'MATHEMATICS']\n",
      "CONCEPTS ['and', 'the', 'operations', 'performed', 'on', 'them', 'Compare', 'MATHEMATICS']\n",
      "LEARNING ['Use', 'only', 'for', 'animal', 'populations', 'Consider', 'also', 'IMITATION']\n",
      "DRUG ['drugs', 'use', 'SIDE', 'EFFECTS']\n",
      "(DRUG) drugs use\n",
      "PTSD ['Posttraumatic', 'stress', 'disorder']\n",
      "(PTSD) Posttraumatic stress disorder\n",
      "DISORDERS ['physical', 'psychological', 'social', 'or', 'situational', 'factors', 'Consider', 'also', 'SUSCEPTIBILITY']\n",
      "LEARNING ['conditioned', 'response', 'was', 'never', 'specifically', 'conditioned', 'Compare', 'GENERALIZATION']\n",
      "LEARNING ['conditioned', 'response', 'was', 'never', 'specifically', 'conditioned', 'Compare', 'GENERALIZATION']\n",
      "EDUCATIONAL ['in', 'reference', 'to', 'some', 'established', 'standard', 'or', 'other', 'criterion', 'Compare', 'GRADING']\n",
      "DRUG ['therapy', 'use', 'SIDE', 'EFFECTS']\n",
      "DRUG ['therapy', 'use', 'SIDE', 'EFFECTS']\n",
      "LEARNING ['property', 'Also', 'known', 'as', 'primary', 'generalization', 'Compare', 'GENERALIZATION']\n",
      "LEARNING ['property', 'Also', 'known', 'as', 'primary', 'generalization', 'Compare', 'GENERALIZATION']\n",
      "RT ['reaction', 'time']\n",
      "(RT) reaction time\n",
      "Hz ['second']\n",
      "TESTING ['describe', 'test', 'performance', 'of', 'individuals', 'Compare', 'SCORING']\n",
      "EDUCATIONAL ['to', 'describe', 'test', 'performance', 'of', 'individuals', 'Compare', 'SCORING', 'TESTING', 'and', 'GRADING']\n",
      "TESTING ['describe', 'test', 'performance', 'of', 'individuals', 'Compare', 'SCORING']\n",
      "EDUCATIONAL ['to', 'describe', 'test', 'performance', 'of', 'individuals', 'Compare', 'SCORING', 'TESTING', 'and', 'GRADING']\n",
      "BODY ['in', 'temperature', 'Compare', 'THERMOREGULATION']\n",
      "BODY ['in', 'temperature', 'Compare', 'THERMOREGULATION']\n",
      "PERCEPTION ['sensory', 'receptor', 'or', 'other', 'neuron', 'will', 'respond', 'Compare', 'SIGNAL', 'DETECTION']\n",
      "PERCEPTION ['sensory', 'receptor', 'or', 'other', 'neuron', 'will', 'respond', 'Compare', 'SIGNAL', 'DETECTION']\n",
      "PERCEPTION ['sensory', 'receptor', 'or', 'other', 'neuron', 'will', 'respond', 'Compare', 'SIGNAL', 'DETECTION']\n",
      "LEARNING ['the', 'prior', 'and', 'current', 'learning', 'situations', 'Compare', 'GENERALIZATION']\n",
      "ASIC ['acid', 'sensing', 'ion', 'channel']\n",
      "(ASIC) acid sensing ion channel\n",
      "ASIC ['acid', 'sensing', 'ion', 'channel']\n",
      "(ASIC) acid sensing ion channel\n",
      "ASIC ['acid', 'sensing', 'ion', 'channel']\n",
      "(ASIC) acid sensing ion channel\n",
      "ASIC ['acid', 'sensing', 'ion', 'channel']\n",
      "(ASIC) acid sensing ion channel\n",
      "ASIC ['acid', 'sensing', 'ion', 'channel']\n",
      "(ASIC) acid sensing ion channel\n",
      "ASIC ['acid', 'sensing', 'ion', 'channel']\n",
      "(ASIC) acid sensing ion channel\n",
      "ADHD ['Attention', 'deficit', 'hyperactivity', 'disorder']\n",
      "(ADHD) Attention deficit hyperactivity disorder\n",
      "ADD ['Attention', 'deficit', 'disorder\"']\n",
      "(ADD) Attention deficit disorder\"\n",
      "ADHD ['Attention', 'deficit', 'hyperactivity', 'disorder']\n",
      "(ADHD) Attention deficit hyperactivity disorder\n",
      "ADD ['Attention', 'deficit', 'disorder\"']\n",
      "(ADD) Attention deficit disorder\"\n",
      "ADHD ['Attention', 'deficit', 'hyperactivity', 'disorder']\n",
      "(ADHD) Attention deficit hyperactivity disorder\n",
      "ADD ['Attention', 'deficit', 'disorder\"']\n",
      "(ADD) Attention deficit disorder\"\n",
      "PMS ['Premenstrual', 'syndrome']\n",
      "(PMS) Premenstrual syndrome\n",
      "SAD ['seasonal', 'affective', 'disorder']\n",
      "(SAD) seasonal affective disorder\n",
      "SAD ['seasonal', 'affective', 'disorder']\n",
      "(SAD) seasonal affective disorder\n",
      "REM ['rapid', 'eye', 'movement']\n",
      "(REM) rapid eye movement\n",
      "PTSD ['Post', 'traumatic', 'stress', 'disorder']\n",
      "(PTSD) Post traumatic stress disorder\n",
      "PTSD ['Post', 'traumatic', 'stress', 'disorder']\n",
      "(PTSD) Post traumatic stress disorder\n",
      "PCP ['use', 'of', 'phencyclidine']\n",
      "PCP ['use', 'of', 'phencyclidine']\n",
      "PCP ['use', 'of', 'phencyclidine']\n",
      "FMA ['An', 'anatomical', 'structure']\n",
      "Trukese ['Chuukese']\n",
      "MSD ['mean', 'squared', 'deviation']\n",
      "(MSD) mean squared deviation\n",
      "DA ['the', 'Army']\n",
      "DA ['the', 'Army']\n",
      "WCST ['Wisconsin', 'Card', 'Sorting', 'Test']\n",
      "(WCST) Wisconsin Card Sorting Test\n",
      "WCST ['Wisconsin', 'Card', 'Sorting', 'Test']\n",
      "(WCST) Wisconsin Card Sorting Test\n",
      "WCST ['Wisconsin', 'Card', 'Sorting', 'Test']\n",
      "(WCST) Wisconsin Card Sorting Test\n",
      "OSD ['Secretary', 'of', 'Defense']\n",
      "(OSD) of Defense\n",
      "OSD ['Secretary', 'of', 'Defense']\n",
      "(OSD) of Defense\n",
      "GMH ['Global', 'Mental', 'Health']\n",
      "(GMH) Global Mental Health\n",
      "GMH ['Global', 'Mental', 'Health']\n",
      "(GMH) Global Mental Health\n",
      "DAF ['the', 'Air', 'Force']\n",
      "DAF ['the', 'Air', 'Force']\n"
     ]
    }
   ],
   "source": [
    "window_size = 10\n",
    "\n",
    "\n",
    "with open('psychThesaurusDict.pkl','rb') as f:\n",
    "        thes = pickle.load(f)      \n",
    "\n",
    "def get_studded_defn(w, dfn):\n",
    "    \n",
    "    w=clean_sent(w.lower())\n",
    "    stud=[w]\n",
    "    dfn=clean_sent(dfn)\n",
    "    tok = word_tokenize(dfn)\n",
    "    for i in range(0, len(tok), window_size):\n",
    "        stud += tok[i:i+window_size]+[w]\n",
    "    return ' '.join(stud)\n",
    "\n",
    "def dict_iter():\n",
    "    for w,d in thes.items():\n",
    "        dfns = d.values()\n",
    "        for dd in dfns:\n",
    "            if len(w)<2:\n",
    "                continue\n",
    "            yield get_studded_defn(w,dd)\n",
    "\n",
    "gen=dict_iter()\n",
    "with open('psychlopedia_train_verbose.txt','w') as f:\n",
    "    for l in gen:\n",
    "        f.write(l+'\\n')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MGI Attention'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_words(abst):\n",
    "    return re.sub(r'([a-zA-Z])[^a-zA-Z]+([a-zA-Z])', r'\\1 \\2', abst)\n",
    "\n",
    "split_words('MGI.|Attention')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "import io\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "def split_words_aggro(abst):\n",
    "    return re.sub(r'([a-zA-Z])[^a-zA-Z]+([a-zA-Z])', r'\\1 \\2', abst) # split words. delims = all non-words\n",
    "\n",
    "def split_words(abst):\n",
    "    return re.sub(r'([a-zA-Z])[\\-\\/]([a-zA-Z])', r'\\1 \\2', abst) # split words, delims = -,/\n",
    "\n",
    "def remove_stopwords(s):\n",
    "# \tnltk_stopwords = ['i',  'me',  'my',  'myself',  'we',  'our',  'ours',  'ourselves',  'you',  \"you're\",  \"you've\",  \"you'll\",  \"you'd\",  'your',  'yours',  'yourself',  'yourselves',  'he',  'him',  'his',  'himself',  'she',  \"she's\",  'her',  'hers',  'herself',  'it',  \"it's\",  'its',  'itself',  'they',  'them',  'their',  'theirs',  'themselves',  'what',  'which',  'who',  'whom',  'this',  'that',  \"that'll\",  'these',  'those',  'am',  'is',  'are',  'was',  'were',  'be',  'been',  'being',  'have',  'has',  'had',  'having',  'do',  'does',  'did',  'doing',  'a',  'an',  'the',  'and',  'but',  'if',  'or',  'because',  'as',  'until',  'while',  'of',  'at',  'by',  'for',  'with',  'about',  'against',  'between',  'into',  'through',  'during',  'before',  'after',  'above',  'below',  'to',  'from',  'up',  'down',  'in',  'out',  'on',  'off',  'over',  'under',  'again',  'further',  'then',  'once',  'here',  'there',  'when',  'where',  'why',  'how',  'all',  'any',  'both',  'each',  'few',  'more',  'most',  'other',  'some',  'such',  'no',  'nor',  'not',  'only',  'own',  'same',  'so',  'than',  'too',  'very',  's',  't',  'can',  'will',  'just',  'don',  \"don't\",  'should',  \"should've\",  'now',  'd',  'll',  'm',  'o',  're',  've',  'y',  'ain',  'aren',  \"aren't\",  'couldn',  \"couldn't\",  'didn',  \"didn't\",  'doesn',  \"doesn't\",  'hadn',  \"hadn't\",  'hasn',  \"hasn't\",  'haven',  \"haven't\",  'isn',  \"isn't\",  'ma',  'mightn',  \"mightn't\",  'mustn',  \"mustn't\",  'needn',  \"needn't\",  'shan',  \"shan't\",  'shouldn',  \"shouldn't\",  'wasn',  \"wasn't\",  'weren',  \"weren't\",  'won',  \"won't\",  'wouldn',  \"wouldn't\"]\n",
    "    nltk_stopwords = stopwords.words('english')\n",
    "    STOPWORDS = nltk_stopwords+[\"'s\", \"’s\"]+[x.capitalize() for x in nltk_stopwords]\n",
    "    tokens = word_tokenize(s)\n",
    "    okstr = ' '.join([x for x in tokens if x in ['(',')'] or x not in STOPWORDS and len(x)>1])\n",
    "    return okstr\n",
    "\n",
    "def remove_punct(s):\n",
    "    s = re.sub(r'[^A-Za-z\\-\\s]+', '', s)\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "def filter_pos_tags(s):\n",
    "    OUT = ['VBD', 'IN', 'VBP', 'RB', 'MD', 'VB', 'VBZ', 'VBG']\n",
    "    return ' '.join([w for w,t in pos_tag(word_tokenize(s)) if t not in OUT])\n",
    "\n",
    "\n",
    "def tokenize(s):\n",
    "    return word_tokenize(s)\n",
    "\n",
    "\n",
    "def abbr_expander(s):   \n",
    "    res = re.findall(r'\\(\\s?[A-Z]\\s?[A-Za-z]+\\s?\\)', s) # Misses acronyms with hyphen\n",
    "           \n",
    "    outs = s\n",
    "    for abbr in res:\n",
    "        ix = s.index(abbr)\n",
    "        abbr_clean = re.sub(r'[^A-Za-z]', '', abbr) # keep only letters, discard all other chars\n",
    "        abbr_len = len(re.sub(r'[a-z]','',abbr_clean)) # Number of capital letters ~ number of tokens to look back for expansion\n",
    "        if abbr_clean[-1]=='s': abbr_len-=1 # Singularing a plural acronym\n",
    "        # When looking for expansion candidates, consider (-) and (/) as word-breaks\n",
    "        candidate_l = split_words_aggro(s[ix-1::-1][::-1].rstrip()).split(' ')[-abbr_len:] # identify possible candidates for expansion from preivous abbr_len words\n",
    "        expansion_l = [] \n",
    "        \n",
    "        aix=0\n",
    "        wix=0\n",
    "        print(abbr_clean, candidate_l)\n",
    "        while aix<len(abbr_clean) and wix<len(candidate_l):\n",
    "            if abbr_clean[aix].lower()==candidate_l[wix][0].lower():\n",
    "                expansion_l.append(candidate_l[wix])\n",
    "                aix+=1\n",
    "                wix+=1\n",
    "            elif abbr[aix].lower() in candidate_l[max(0,wix-1)].lower():\n",
    "                aix+=1\n",
    "            else:\n",
    "                wix+=1\n",
    "\n",
    "        if len(expansion_l)>0.4*abbr_len:\n",
    "            expanded = ' '.join(expansion_l)\n",
    "            outs = outs.replace(' '+abbr, '') # Remove first instance of abbreviation before expanding other instances\n",
    "            if abbr_clean[-1]=='s':\n",
    "                outs = outs.replace(abbr_clean[:-1], expanded)\n",
    "            else:\n",
    "                outs = outs.replace(abbr_clean, expanded) \n",
    "            print(abbr,expanded)\n",
    "\n",
    "    return outs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sim(v1, v2):\n",
    "    return 1-cosine(v1,v2)\n",
    "\n",
    "\n",
    "\n",
    "def idf(doclist):\n",
    "    N = len(doclist)\n",
    "    vocab = list(FreqDist(word_tokenize(' '.join(doclist))).keys())\n",
    "    idfd={}\n",
    "    for w in vocab:\n",
    "        idfd[w] = math.log(N/max(1,sum([1 for d in doclist if w in d])), 10)\n",
    "    return idfd\n",
    "\n",
    "def tfidfDist(doclist):\n",
    "    idf_lookup = idf(doclist)\n",
    "    docs_tfidf = []\n",
    "    for doc in doclist:\n",
    "        token_fd = list(FreqDist(word_tokenize(doc)).items())\n",
    "        tfidf = [(w, freq*idf_lookup[w]) for w,freq in token_fd]\n",
    "        docs_tfidf.append(sorted(tfidf, key=lambda x:x[1], reverse=True))\n",
    "    return docs_tfidf\n",
    "\n",
    "\n",
    "def filter_bottom_tfidf(doclist, cutoff_pcile=0.2):\n",
    "    tfidf_vals = tfidfDist(doclist)\n",
    "    filt_docs=[]\n",
    "\n",
    "    for c,doc in enumerate(doclist):\n",
    "        tf = tfidf_vals[c]\n",
    "        allowed = [w for w,n in tf[:math.ceil((1-cutoff_pcile)*len(tf))]]\n",
    "        filt_docs.append(' '.join([w for w in word_tokenize(doc) if w in allowed]))\n",
    "\n",
    "    return filt_docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sdfds SDFSDF Sdcxcsqw'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def c(abst):\n",
    "    abst = split_words(abst)\n",
    "    abst = abbr_expander(abst)\n",
    "    \n",
    "    # Make post-period and first word uppercase chars lower\n",
    "    abst = re.sub(r\"(?<=\\. )[A-Z]\",lambda t:t.group().lower(), abst)\n",
    "    abst = abst[0].lower()+abst[1:]\n",
    "#     abst = remove_punct(abst)\n",
    "    \n",
    "#     abst = filter_pos_tags(abst)\n",
    "#     abst=remove_stopwords(abst)\n",
    "\n",
    "    return abst\n",
    "\n",
    "\n",
    "c(\"sdfds SDFSDF Sdcxcsqw\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "143.5px",
    "left": "997px",
    "right": "20px",
    "top": "122px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
