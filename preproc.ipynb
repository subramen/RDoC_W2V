{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = open('abstracts.csv','r',  encoding='utf-8')\n",
    "out = []\n",
    "\n",
    "for l in txt:\n",
    "    try:\n",
    "        comma = l.index(',')\n",
    "    except ValueError:\n",
    "        continue\n",
    "    pmid = l[:comma]\n",
    "    content = l[comma+1:]\n",
    "    out.append(pmid+'\\t'+content)\n",
    "    \n",
    "        \n",
    "with open('abstracts_kv.tsv','w',encoding='utf-8') as f:\n",
    "    f.writelines(out)\n",
    "txt.close()\n",
    "\n",
    "df = pd.read_csv('abstracts_kv.tsv',sep='\\t')\n",
    "df.columns = ['pmid', 'content']\n",
    "df2 = df.content.str.split('\\.,').apply(pd.Series, 1)\n",
    "8180 pmids have multiple delims, ignoring them for now\n",
    "df2 = df2[[0,1]]\n",
    "df2.columns = ['title','abstract']\n",
    "df2['pmid'] = df['pmid']\n",
    "\n",
    "#handle abstracs which are null because title<->abstract delimiter was not .,\n",
    "df2['abstract'] = np.where(df2.abstract.isnull(), df2.title, df2.abstract)\n",
    "df3 = df2.dropna(how='any')\n",
    "\n",
    "# combine sectioned abstracts\n",
    "s=df3.pmid.value_counts()\n",
    "mult_pmid = df3[df3.pmid.isin(s.index[s>1])]\n",
    "single_pmid = df3[~df3.pmid.isin(s.index[s>1])]\n",
    "\n",
    "mult_pmid2 = mult_pmid.groupby(['pmid']).abstract.transform(lambda x: ' '.join(x))\n",
    "mult_pmid['abstract'] = mult_pmid2\n",
    "mult_pmid = mult_pmid.drop_duplicates(subset=['pmid','abstract'])\n",
    "\n",
    "data_df = single_pmid.append(mult_pmid)\n",
    "data_df['content'] = np.where(data_df.title!=data_df.abstract, data_df.title+' '+data_df.abstract, data_df.abstract)\n",
    "\n",
    "# data_df.to_pickle('data_df.pkl')\n",
    "\n",
    "data = data_df[['pmid', 'content']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stopwords\n",
    "Hyphens - While generating hyperspace, tokenize word into nonhyphenated constituents. When looking up, sum up vecs of each constituent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = stopwords.words('english')+[\"'s\", \"â€™s\"]\n",
    "def remove_stopwords(s):\n",
    "    s = re.sub(r'([a-zA-Z])[\\-\\/]([a-zA-Z])', r'\\1 \\2', s) # split hyphenated and slashed words\n",
    "    s = re.sub(r'[^A-Za-z\\s\\(\\)]+', '', s)\n",
    "    tokens = word_tokenize(s)\n",
    "    okstr = ' '.join([x for x in tokens if x in ['(',')'] or x not in STOPWORDS and len(x)>1])\n",
    "    #only remove parentheses which are empty\n",
    "    okstr = re.sub(r'\\(\\s*\\)','',okstr)\n",
    "    return okstr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Abbreviation in Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABBR_LIST = defaultdict(list)\n",
    "\n",
    "def abbr_expander(s):\n",
    "    res = re.findall(r'\\(\\s?[A-Z]\\s?[A-Za-z]+\\s?\\)', s) # Misses acronyms with hyphen\n",
    "    outs = s\n",
    "    retstr = ''\n",
    "    for abbr in res:\n",
    "        ix = s.index(abbr)\n",
    "        abbr_clean = re.sub(r'[^A-Za-z]', '', abbr) # keep only letters, discard all other chars\n",
    "        abbr_len = len(re.sub(r'[a-z]','',abbr_clean)) # Number of capital letters ~ number of tokens to look back for expansion\n",
    "        if abbr_clean[-1]=='s': abbr_len-=1\n",
    "        # When looking for expansion candidates, consider (-) and (/) as word-breaks\n",
    "        expanded = ' '.join(s[ix-1:0:-1][::-1].rstrip().split(' ')[-abbr_len:])\n",
    "        ABBR_LIST[abbr_clean].append(expanded)\n",
    "        # Remove first instance of abbreviation before expanding other instances\n",
    "        outs = outs.replace(abbr_clean, expanded)\n",
    "        if abbr_clean[-1]=='s':\n",
    "            outs = outs.replace(abbr_clean[:-1], expanded)\n",
    "    return outs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get doc-per-line file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_line_generator():\n",
    "    ABBR_LIST = defaultdict(list)\n",
    "    for abst in iter(data_df.content):\n",
    "        abst1 = remove_stopwords(abst)\n",
    "        outs = abbr_expander(abst1)\n",
    "        outs = re.sub(r'[\\(\\)]', '', outs) # shifted both of them here out of abbr_expander() because it wasn't working there for some reason\n",
    "        outs = re.sub(r'\\s{2,}', ' ', outs)\n",
    "        f.write(outs+'\\n')\n",
    "    \n",
    "with open('one-abstract-per-line.txt','w') as f:\n",
    "    clean_line_generator()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
